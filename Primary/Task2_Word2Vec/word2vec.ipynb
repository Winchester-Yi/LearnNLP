{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Author: Shiva Manne <manneshiva@gmail.com>\n",
    "# Copyright (C) 2018 RaRe Technologies s.r.o.\n",
    "# Licensed under the GNU LGPL v2.1 - http://www.gnu.org/licenses/lgpl.html\n",
    "\n",
    "from __future__ import division  # py3 \"true division\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import heapq\n",
    "from timeit import default_timer\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "from gensim.utils import keep_vocab_item, call_on_class_only\n",
    "from gensim.models.keyedvectors import Vocab, Word2VecKeyedVectors\n",
    "from gensim.models.base_any2vec import BaseWordEmbeddingsModel\n",
    "\n",
    "try:\n",
    "    from queue import Queue, Empty\n",
    "except ImportError:\n",
    "    from Queue import Queue, Empty\n",
    "\n",
    "from numpy import exp, dot, zeros, random, dtype, float32 as REAL,\\\n",
    "    uint32, seterr, array, uint8, vstack, fromstring, sqrt,\\\n",
    "    empty, sum as np_sum, ones, logaddexp, log, outer\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "from gensim import utils, matutils  # utility fnc for pickling, common scipy operations etc\n",
    "from gensim.utils import deprecated\n",
    "from six import iteritems, itervalues, string_types\n",
    "from six.moves import range\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    from gensim.models.word2vec_inner import train_batch_sg, train_batch_cbow\n",
    "    from gensim.models.word2vec_inner import score_sentence_sg, score_sentence_cbow\n",
    "    from gensim.models.word2vec_inner import FAST_VERSION, MAX_WORDS_IN_BATCH\n",
    "\n",
    "except ImportError:\n",
    "    # failed... fall back to plain numpy (20-80x slower training than the above)\n",
    "    FAST_VERSION = -1\n",
    "    MAX_WORDS_IN_BATCH = 10000\n",
    "\n",
    "    def train_batch_sg(model, sentences, alpha, work=None, compute_loss=False):\n",
    "        \"\"\"Update skip-gram model by training on a sequence of sentences.\n",
    "        Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
    "        Warnings\n",
    "        --------\n",
    "        This is the non-optimized, pure Python version. If you have a C compiler, Gensim\n",
    "        will use an optimized code path from :mod:`gensim.models.word2vec_inner` instead.\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : :class:`~gensim.models.word2Vec.Word2Vec`\n",
    "            The Word2Vec model instance to train.\n",
    "        sentences : iterable of list of str\n",
    "            The corpus used to train the model.\n",
    "        alpha : float\n",
    "            The learning rate\n",
    "        work : object, optional\n",
    "            Unused.\n",
    "        compute_loss : bool, optional\n",
    "            Whether or not the training loss should be computed in this batch.\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of words in the vocabulary actually used for training (that already existed in the vocabulary\n",
    "            and were not discarded by negative sampling).\n",
    "        \"\"\"\n",
    "        result = 0\n",
    "        for sentence in sentences:\n",
    "            word_vocabs = [model.wv.vocab[w] for w in sentence if w in model.wv.vocab\n",
    "                           and model.wv.vocab[w].sample_int > model.random.rand() * 2 ** 32]\n",
    "            for pos, word in enumerate(word_vocabs):\n",
    "                reduced_window = model.random.randint(model.window)  # `b` in the original word2vec code\n",
    "\n",
    "                # now go over all words from the (reduced) window, predicting each one in turn\n",
    "                start = max(0, pos - model.window + reduced_window)\n",
    "                for pos2, word2 in enumerate(word_vocabs[start:(pos + model.window + 1 - reduced_window)], start):\n",
    "                    # don't train on the `word` itself\n",
    "                    if pos2 != pos:\n",
    "                        train_sg_pair(\n",
    "                            model, model.wv.index2word[word.index], word2.index, alpha, compute_loss=compute_loss\n",
    "                        )\n",
    "\n",
    "            result += len(word_vocabs)\n",
    "        return result\n",
    "\n",
    "    def train_batch_cbow(model, sentences, alpha, work=None, neu1=None, compute_loss=False):\n",
    "        \"\"\"Update CBOW model by training on a sequence of sentences.\n",
    "        Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
    "        Warnings\n",
    "        --------\n",
    "        This is the non-optimized, pure Python version. If you have a C compiler, Gensim\n",
    "        will use an optimized code path from :mod:`gensim.models.word2vec_inner` instead.\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
    "            The Word2Vec model instance to train.\n",
    "        sentences : iterable of list of str\n",
    "            The corpus used to train the model.\n",
    "        alpha : float\n",
    "            The learning rate\n",
    "        work : object, optional\n",
    "            Unused.\n",
    "        neu1 : object, optional\n",
    "            Unused.\n",
    "        compute_loss : bool, optional\n",
    "            Whether or not the training loss should be computed in this batch.\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of words in the vocabulary actually used for training (that already existed in the vocabulary\n",
    "            and were not discarded by negative sampling).\n",
    "        \"\"\"\n",
    "        result = 0\n",
    "        for sentence in sentences:\n",
    "            word_vocabs = [\n",
    "                model.wv.vocab[w] for w in sentence if w in model.wv.vocab\n",
    "                and model.wv.vocab[w].sample_int > model.random.rand() * 2 ** 32\n",
    "            ]\n",
    "            for pos, word in enumerate(word_vocabs):\n",
    "                reduced_window = model.random.randint(model.window)  # `b` in the original word2vec code\n",
    "                start = max(0, pos - model.window + reduced_window)\n",
    "                window_pos = enumerate(word_vocabs[start:(pos + model.window + 1 - reduced_window)], start)\n",
    "                word2_indices = [word2.index for pos2, word2 in window_pos if (word2 is not None and pos2 != pos)]\n",
    "                l1 = np_sum(model.wv.syn0[word2_indices], axis=0)  # 1 x vector_size\n",
    "                if word2_indices and model.cbow_mean:\n",
    "                    l1 /= len(word2_indices)\n",
    "                train_cbow_pair(model, word, word2_indices, l1, alpha, compute_loss=compute_loss)\n",
    "            result += len(word_vocabs)\n",
    "        return result\n",
    "\n",
    "    def score_sentence_sg(model, sentence, work=None):\n",
    "        \"\"\"Obtain likelihood score for a single sentence in a fitted skip-gram representation.\n",
    "        Notes\n",
    "        -----\n",
    "        This is the non-optimized, pure Python version. If you have a C compiler, Gensim\n",
    "        will use an optimized code path from :mod:`gensim.models.word2vec_inner` instead.\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
    "            The trained model. It **MUST** have been trained using hierarchical softmax and the skip-gram algorithm.\n",
    "        sentence : list of str\n",
    "            The words comprising the sentence to be scored.\n",
    "        work : object, optional\n",
    "            Unused. For interface compatibility only.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The probability assigned to this sentence by the Skip-Gram model.\n",
    "        \"\"\"\n",
    "        log_prob_sentence = 0.0\n",
    "        if model.negative:\n",
    "            raise RuntimeError(\"scoring is only available for HS=True\")\n",
    "\n",
    "        word_vocabs = [model.wv.vocab[w] for w in sentence if w in model.wv.vocab]\n",
    "        for pos, word in enumerate(word_vocabs):\n",
    "            if word is None:\n",
    "                continue  # OOV word in the input sentence => skip\n",
    "\n",
    "            # now go over all words from the window, predicting each one in turn\n",
    "            start = max(0, pos - model.window)\n",
    "            for pos2, word2 in enumerate(word_vocabs[start: pos + model.window + 1], start):\n",
    "                # don't train on OOV words and on the `word` itself\n",
    "                if word2 is not None and pos2 != pos:\n",
    "                    log_prob_sentence += score_sg_pair(model, word, word2)\n",
    "\n",
    "        return log_prob_sentence\n",
    "\n",
    "    def score_sentence_cbow(model, sentence, work=None, neu1=None):\n",
    "        \"\"\"Obtain likelihood score for a single sentence in a fitted CBOW representation.\n",
    "        Notes\n",
    "        -----\n",
    "        This is the non-optimized, pure Python version. If you have a C compiler, Gensim\n",
    "        will use an optimized code path from :mod:`gensim.models.word2vec_inner` instead.\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : :class:`~gensim.models.word2vec.Word2Vec`\n",
    "            The trained model. It **MUST** have been trained using hierarchical softmax and the CBOW algorithm.\n",
    "        sentence : list of str\n",
    "            The words comprising the sentence to be scored.\n",
    "        work : object, optional\n",
    "            Unused. For interface compatibility only.\n",
    "        neu1 : object, optional\n",
    "            Unused. For interface compatibility only.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The probability assigned to this sentence by the CBOW model.\n",
    "        \"\"\"\n",
    "        log_prob_sentence = 0.0\n",
    "        if model.negative:\n",
    "            raise RuntimeError(\"scoring is only available for HS=True\")\n",
    "\n",
    "        word_vocabs = [model.wv.vocab[w] for w in sentence if w in model.wv.vocab]\n",
    "        for pos, word in enumerate(word_vocabs):\n",
    "            if word is None:\n",
    "                continue  # OOV word in the input sentence => skip\n",
    "\n",
    "            start = max(0, pos - model.window)\n",
    "            window_pos = enumerate(word_vocabs[start:(pos + model.window + 1)], start)\n",
    "            word2_indices = [word2.index for pos2, word2 in window_pos if (word2 is not None and pos2 != pos)]\n",
    "            l1 = np_sum(model.wv.syn0[word2_indices], axis=0)  # 1 x layer1_size\n",
    "            if word2_indices and model.cbow_mean:\n",
    "                l1 /= len(word2_indices)\n",
    "            log_prob_sentence += score_cbow_pair(model, word, l1)\n",
    "\n",
    "        return log_prob_sentence\n",
    "\n",
    "try:\n",
    "    from gensim.models.word2vec_corpusfile import train_epoch_sg, train_epoch_cbow, CORPUSFILE_VERSION\n",
    "except ImportError:\n",
    "    # file-based word2vec is not supported\n",
    "    CORPUSFILE_VERSION = -1\n",
    "\n",
    "    def train_epoch_sg(model, corpus_file, offset, _cython_vocab, _cur_epoch, _expected_examples, _expected_words,\n",
    "                       _work, _neu1, compute_loss):\n",
    "        raise RuntimeError(\"Training with corpus_file argument is not supported\")\n",
    "\n",
    "    def train_epoch_cbow(model, corpus_file, offset, _cython_vocab, _cur_epoch, _expected_examples, _expected_words,\n",
    "                         _work, _neu1, compute_loss):\n",
    "        raise RuntimeError(\"Training with corpus_file argument is not supported\")\n",
    "\n",
    "\n",
    "def train_sg_pair(model, word, context_index, alpha, learn_vectors=True, learn_hidden=True,\n",
    "                  context_vectors=None, context_locks=None, compute_loss=False, is_ft=False):\n",
    "    \"\"\"Train the passed model instance on a word and its context, using the Skip-gram algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : :class:`~gensim.models.word2vec.Word2Vec`\n",
    "        The model to be trained.\n",
    "    word : str\n",
    "        The label (predicted) word.\n",
    "    context_index : list of int\n",
    "        The vocabulary indices of the words in the context.\n",
    "    alpha : float\n",
    "        Learning rate.\n",
    "    learn_vectors : bool, optional\n",
    "        Whether the vectors should be updated.\n",
    "    learn_hidden : bool, optional\n",
    "        Whether the weights of the hidden layer should be updated.\n",
    "    context_vectors : list of list of float, optional\n",
    "        Vector representations of the words in the context. If None, these will be retrieved from the model.\n",
    "    context_locks : list of float, optional\n",
    "        The lock factors for each word in the context.\n",
    "    compute_loss : bool, optional\n",
    "        Whether or not the training loss should be computed.\n",
    "    is_ft : bool, optional\n",
    "        If True, weights will be computed using `model.wv.syn0_vocab` and `model.wv.syn0_ngrams`\n",
    "        instead of `model.wv.syn0`.\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Error vector to be back-propagated.\n",
    "    \"\"\"\n",
    "    if context_vectors is None:\n",
    "        if is_ft:\n",
    "            context_vectors_vocab = model.wv.syn0_vocab\n",
    "            context_vectors_ngrams = model.wv.syn0_ngrams\n",
    "        else:\n",
    "            context_vectors = model.wv.syn0\n",
    "    if context_locks is None:\n",
    "        if is_ft:\n",
    "            context_locks_vocab = model.syn0_vocab_lockf\n",
    "            context_locks_ngrams = model.syn0_ngrams_lockf\n",
    "        else:\n",
    "            context_locks = model.syn0_lockf\n",
    "\n",
    "    if word not in model.wv.vocab:\n",
    "        return\n",
    "    predict_word = model.wv.vocab[word]  # target word (NN output)\n",
    "\n",
    "    if is_ft:\n",
    "        l1_vocab = context_vectors_vocab[context_index[0]]\n",
    "        l1_ngrams = np_sum(context_vectors_ngrams[context_index[1:]], axis=0)\n",
    "        if context_index:\n",
    "            l1 = np_sum([l1_vocab, l1_ngrams], axis=0) / len(context_index)\n",
    "    else:\n",
    "        l1 = context_vectors[context_index]  # input word (NN input/projection layer)\n",
    "        lock_factor = context_locks[context_index]\n",
    "\n",
    "    neu1e = zeros(l1.shape)\n",
    "\n",
    "    if model.hs:\n",
    "        # work on the entire tree at once, to push as much work into numpy's C routines as possible (performance)\n",
    "        l2a = deepcopy(model.syn1[predict_word.point])  # 2d matrix, codelen x layer1_size\n",
    "        prod_term = dot(l1, l2a.T)\n",
    "        fa = expit(prod_term)  # propagate hidden -> output\n",
    "        ga = (1 - predict_word.code - fa) * alpha  # vector of error gradients multiplied by the learning rate\n",
    "        if learn_hidden:\n",
    "            model.syn1[predict_word.point] += outer(ga, l1)  # learn hidden -> output\n",
    "        neu1e += dot(ga, l2a)  # save error\n",
    "\n",
    "        # loss component corresponding to hierarchical softmax\n",
    "        if compute_loss:\n",
    "            sgn = (-1.0) ** predict_word.code  # `ch` function, 0 -> 1, 1 -> -1\n",
    "            lprob = -log(expit(-sgn * prod_term))\n",
    "            model.running_training_loss += sum(lprob)\n",
    "\n",
    "    if model.negative:\n",
    "        # use this word (label = 1) + `negative` other random words not from this sentence (label = 0)\n",
    "        word_indices = [predict_word.index]\n",
    "        while len(word_indices) < model.negative + 1:\n",
    "            w = model.cum_table.searchsorted(model.random.randint(model.cum_table[-1]))\n",
    "            if w != predict_word.index:\n",
    "                word_indices.append(w)\n",
    "        l2b = model.syn1neg[word_indices]  # 2d matrix, k+1 x layer1_size\n",
    "        prod_term = dot(l1, l2b.T)\n",
    "        fb = expit(prod_term)  # propagate hidden -> output\n",
    "        gb = (model.neg_labels - fb) * alpha  # vector of error gradients multiplied by the learning rate\n",
    "        if learn_hidden:\n",
    "            model.syn1neg[word_indices] += outer(gb, l1)  # learn hidden -> output\n",
    "        neu1e += dot(gb, l2b)  # save error\n",
    "\n",
    "        # loss component corresponding to negative sampling\n",
    "        if compute_loss:\n",
    "            model.running_training_loss -= sum(log(expit(-1 * prod_term[1:])))  # for the sampled words\n",
    "            model.running_training_loss -= log(expit(prod_term[0]))  # for the output word\n",
    "\n",
    "    if learn_vectors:\n",
    "        if is_ft:\n",
    "            model.wv.syn0_vocab[context_index[0]] += neu1e * context_locks_vocab[context_index[0]]\n",
    "            for i in context_index[1:]:\n",
    "                model.wv.syn0_ngrams[i] += neu1e * context_locks_ngrams[i]\n",
    "        else:\n",
    "            l1 += neu1e * lock_factor  # learn input -> hidden (mutates model.wv.syn0[word2.index], if that is l1)\n",
    "    return neu1e\n",
    "\n",
    "\n",
    "def train_cbow_pair(model, word, input_word_indices, l1, alpha, learn_vectors=True, learn_hidden=True,\n",
    "                    compute_loss=False, context_vectors=None, context_locks=None, is_ft=False):\n",
    "    \"\"\"Train the passed model instance on a word and its context, using the CBOW algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : :class:`~gensim.models.word2vec.Word2Vec`\n",
    "        The model to be trained.\n",
    "    word : str\n",
    "        The label (predicted) word.\n",
    "    input_word_indices : list of int\n",
    "        The vocabulary indices of the words in the context.\n",
    "    l1 : list of float\n",
    "        Vector representation of the label word.\n",
    "    alpha : float\n",
    "        Learning rate.\n",
    "    learn_vectors : bool, optional\n",
    "        Whether the vectors should be updated.\n",
    "    learn_hidden : bool, optional\n",
    "        Whether the weights of the hidden layer should be updated.\n",
    "    compute_loss : bool, optional\n",
    "        Whether or not the training loss should be computed.\n",
    "    context_vectors : list of list of float, optional\n",
    "        Vector representations of the words in the context. If None, these will be retrieved from the model.\n",
    "    context_locks : list of float, optional\n",
    "        The lock factors for each word in the context.\n",
    "    is_ft : bool, optional\n",
    "        If True, weights will be computed using `model.wv.syn0_vocab` and `model.wv.syn0_ngrams`\n",
    "        instead of `model.wv.syn0`.\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Error vector to be back-propagated.\n",
    "    \"\"\"\n",
    "    if context_vectors is None:\n",
    "        if is_ft:\n",
    "            context_vectors_vocab = model.wv.syn0_vocab\n",
    "            context_vectors_ngrams = model.wv.syn0_ngrams\n",
    "        else:\n",
    "            context_vectors = model.wv.syn0\n",
    "    if context_locks is None:\n",
    "        if is_ft:\n",
    "            context_locks_vocab = model.syn0_vocab_lockf\n",
    "            context_locks_ngrams = model.syn0_ngrams_lockf\n",
    "        else:\n",
    "            context_locks = model.syn0_lockf\n",
    "\n",
    "    neu1e = zeros(l1.shape)\n",
    "\n",
    "    if model.hs:\n",
    "        l2a = model.syn1[word.point]  # 2d matrix, codelen x layer1_size\n",
    "        prod_term = dot(l1, l2a.T)\n",
    "        fa = expit(prod_term)  # propagate hidden -> output\n",
    "        ga = (1. - word.code - fa) * alpha  # vector of error gradients multiplied by the learning rate\n",
    "        if learn_hidden:\n",
    "            model.syn1[word.point] += outer(ga, l1)  # learn hidden -> output\n",
    "        neu1e += dot(ga, l2a)  # save error\n",
    "\n",
    "        # loss component corresponding to hierarchical softmax\n",
    "        if compute_loss:\n",
    "            sgn = (-1.0) ** word.code  # ch function, 0-> 1, 1 -> -1\n",
    "            model.running_training_loss += sum(-log(expit(-sgn * prod_term)))\n",
    "\n",
    "    if model.negative:\n",
    "        # use this word (label = 1) + `negative` other random words not from this sentence (label = 0)\n",
    "        word_indices = [word.index]\n",
    "        while len(word_indices) < model.negative + 1:\n",
    "            w = model.cum_table.searchsorted(model.random.randint(model.cum_table[-1]))\n",
    "            if w != word.index:\n",
    "                word_indices.append(w)\n",
    "        l2b = model.syn1neg[word_indices]  # 2d matrix, k+1 x layer1_size\n",
    "        prod_term = dot(l1, l2b.T)\n",
    "        fb = expit(prod_term)  # propagate hidden -> output\n",
    "        gb = (model.neg_labels - fb) * alpha  # vector of error gradients multiplied by the learning rate\n",
    "        if learn_hidden:\n",
    "            model.syn1neg[word_indices] += outer(gb, l1)  # learn hidden -> output\n",
    "        neu1e += dot(gb, l2b)  # save error\n",
    "\n",
    "        # loss component corresponding to negative sampling\n",
    "        if compute_loss:\n",
    "            model.running_training_loss -= sum(log(expit(-1 * prod_term[1:])))  # for the sampled words\n",
    "            model.running_training_loss -= log(expit(prod_term[0]))  # for the output word\n",
    "\n",
    "    if learn_vectors:\n",
    "        # learn input -> hidden, here for all words in the window separately\n",
    "        if is_ft:\n",
    "            if not model.cbow_mean and input_word_indices:\n",
    "                neu1e /= (len(input_word_indices[0]) + len(input_word_indices[1]))\n",
    "            for i in input_word_indices[0]:\n",
    "                context_vectors_vocab[i] += neu1e * context_locks_vocab[i]\n",
    "            for i in input_word_indices[1]:\n",
    "                context_vectors_ngrams[i] += neu1e * context_locks_ngrams[i]\n",
    "        else:\n",
    "            if not model.cbow_mean and input_word_indices:\n",
    "                neu1e /= len(input_word_indices)\n",
    "            for i in input_word_indices:\n",
    "                context_vectors[i] += neu1e * context_locks[i]\n",
    "\n",
    "    return neu1e\n",
    "\n",
    "\n",
    "def score_sg_pair(model, word, word2):\n",
    "    \"\"\"Score the trained Skip-gram model on a pair of words.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : :class:`~gensim.models.word2vec.Word2Vec`\n",
    "        The trained model.\n",
    "    word : :class:`~gensim.models.keyedvectors.Vocab`\n",
    "        Vocabulary representation of the first word.\n",
    "    word2 : :class:`~gensim.models.keyedvectors.Vocab`\n",
    "        Vocabulary representation of the second word.\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Logarithm of the sum of exponentiations of input words.\n",
    "    \"\"\"\n",
    "    l1 = model.wv.syn0[word2.index]\n",
    "    l2a = deepcopy(model.syn1[word.point])  # 2d matrix, codelen x layer1_size\n",
    "    sgn = (-1.0) ** word.code  # ch function, 0-> 1, 1 -> -1\n",
    "    lprob = -logaddexp(0, -sgn * dot(l1, l2a.T))\n",
    "    return sum(lprob)\n",
    "\n",
    "\n",
    "def score_cbow_pair(model, word, l1):\n",
    "    \"\"\"Score the trained CBOW model on a pair of words.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : :class:`~gensim.models.word2vec.Word2Vec`\n",
    "        The trained model.\n",
    "    word : :class:`~gensim.models.keyedvectors.Vocab`\n",
    "        Vocabulary representation of the first word.\n",
    "    l1 : list of float\n",
    "        Vector representation of the second word.\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Logarithm of the sum of exponentiations of input words.\n",
    "    \"\"\"\n",
    "    l2a = model.syn1[word.point]  # 2d matrix, codelen x layer1_size\n",
    "    sgn = (-1.0) ** word.code  # ch function, 0-> 1, 1 -> -1\n",
    "    lprob = -logaddexp(0, -sgn * dot(l1, l2a.T))\n",
    "    return sum(lprob)\n",
    "\n",
    "\n",
    "class Word2Vec(BaseWordEmbeddingsModel):\n",
    "    \"\"\"Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
    "    Once you're finished training a model (=no more updates, only querying)\n",
    "    store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
    "    The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
    "    :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
    "    The trained word vectors can also be stored/loaded from a format compatible with the\n",
    "    original word2vec implementation via `self.wv.save_word2vec_format`\n",
    "    and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
    "    Some important attributes are the following:\n",
    "    Attributes\n",
    "    ----------\n",
    "    wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
    "        This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
    "        directly to query those embeddings in various ways. See the module level docstring for examples.\n",
    "    vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
    "        This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
    "        Besides keeping track of all unique words, this object provides extra functionality, such as\n",
    "        constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
    "    trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
    "        This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
    "        network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
    "        a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
    "        (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5,\n",
    "                 max_vocab_size=None, sample=1e-3, seed=1, workers=3, min_alpha=0.0001,\n",
    "                 sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=hash, iter=5, null_word=0,\n",
    "                 trim_rule=None, sorted_vocab=1, batch_words=MAX_WORDS_IN_BATCH, compute_loss=False, callbacks=(),\n",
    "                 max_final_vocab=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        sentences : iterable of iterables, optional\n",
    "            The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
    "            consider an iterable that streams the sentences directly from disk/network.\n",
    "            See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
    "            or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
    "            See also the `tutorial on data streaming in Python\n",
    "            <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
    "            If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
    "            in some other way.\n",
    "        corpus_file : str, optional\n",
    "            Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
    "            You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
    "            `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
    "        size : int, optional\n",
    "            Dimensionality of the word vectors.\n",
    "        window : int, optional\n",
    "            Maximum distance between the current and predicted word within a sentence.\n",
    "        min_count : int, optional\n",
    "            Ignores all words with total frequency lower than this.\n",
    "        workers : int, optional\n",
    "            Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "        sg : {0, 1}, optional\n",
    "            Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "        hs : {0, 1}, optional\n",
    "            If 1, hierarchical softmax will be used for model training.\n",
    "            If 0, and `negative` is non-zero, negative sampling will be used.\n",
    "        negative : int, optional\n",
    "            If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
    "            should be drawn (usually between 5-20).\n",
    "            If set to 0, no negative sampling is used.\n",
    "        ns_exponent : float, optional\n",
    "            The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
    "            to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
    "            than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
    "            More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
    "            other values may perform better for recommendation applications.\n",
    "        cbow_mean : {0, 1}, optional\n",
    "            If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
    "        alpha : float, optional\n",
    "            The initial learning rate.\n",
    "        min_alpha : float, optional\n",
    "            Learning rate will linearly drop to `min_alpha` as training progresses.\n",
    "        seed : int, optional\n",
    "            Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
    "            the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
    "            you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
    "            from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
    "            use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
    "        max_vocab_size : int, optional\n",
    "            Limits the RAM during vocabulary building; if there are more unique\n",
    "            words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
    "            Set to `None` for no limit.\n",
    "        max_final_vocab : int, optional\n",
    "            Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
    "            min_count is more than the calculated min_count, the specified min_count will be used.\n",
    "            Set to `None` if not required.\n",
    "        sample : float, optional\n",
    "            The threshold for configuring which higher-frequency words are randomly downsampled,\n",
    "            useful range is (0, 1e-5).\n",
    "        hashfxn : function, optional\n",
    "            Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
    "        iter : int, optional\n",
    "            Number of iterations (epochs) over the corpus.\n",
    "        trim_rule : function, optional\n",
    "            Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
    "            be trimmed away, or handled using the default (discard if word count < min_count).\n",
    "            Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
    "            or a callable that accepts parameters (word, count, min_count) and returns either\n",
    "            :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
    "            The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
    "            model.\n",
    "            The input parameters are of the following types:\n",
    "                * `word` (str) - the word we are examining\n",
    "                * `count` (int) - the word's frequency count in the corpus\n",
    "                * `min_count` (int) - the minimum count threshold.\n",
    "        sorted_vocab : {0, 1}, optional\n",
    "            If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
    "            See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
    "        batch_words : int, optional\n",
    "            Target size (in words) for batches of examples passed to worker threads (and\n",
    "            thus cython routines).(Larger batches will be passed if individual\n",
    "            texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
    "        compute_loss: bool, optional\n",
    "            If True, computes and stores loss value which can be retrieved using\n",
    "            :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
    "        callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
    "            Sequence of callbacks to be executed at specific stages during training.\n",
    "        Examples\n",
    "        --------\n",
    "        Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
    "        .. sourcecode:: pycon\n",
    "            >>> from gensim.models import Word2Vec\n",
    "            >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "            >>> model = Word2Vec(sentences, min_count=1)\n",
    "        \"\"\"\n",
    "        self.max_final_vocab = max_final_vocab\n",
    "\n",
    "        self.callbacks = callbacks\n",
    "        self.load = call_on_class_only\n",
    "\n",
    "        self.wv = Word2VecKeyedVectors(size)\n",
    "        self.vocabulary = Word2VecVocab(\n",
    "            max_vocab_size=max_vocab_size, min_count=min_count, sample=sample, sorted_vocab=bool(sorted_vocab),\n",
    "            null_word=null_word, max_final_vocab=max_final_vocab, ns_exponent=ns_exponent)\n",
    "        self.trainables = Word2VecTrainables(seed=seed, vector_size=size, hashfxn=hashfxn)\n",
    "\n",
    "        super(Word2Vec, self).__init__(\n",
    "            sentences=sentences, corpus_file=corpus_file, workers=workers, vector_size=size, epochs=iter,\n",
    "            callbacks=callbacks, batch_words=batch_words, trim_rule=trim_rule, sg=sg, alpha=alpha, window=window,\n",
    "            seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss,\n",
    "            fast_version=FAST_VERSION)\n",
    "\n",
    "    def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
    "                        total_examples=None, total_words=None, **kwargs):\n",
    "        work, neu1 = thread_private_mem\n",
    "\n",
    "        if self.sg:\n",
    "            examples, tally, raw_tally = train_epoch_sg(self, corpus_file, offset, cython_vocab, cur_epoch,\n",
    "                                                        total_examples, total_words, work, neu1, self.compute_loss)\n",
    "        else:\n",
    "            examples, tally, raw_tally = train_epoch_cbow(self, corpus_file, offset, cython_vocab, cur_epoch,\n",
    "                                                          total_examples, total_words, work, neu1, self.compute_loss)\n",
    "\n",
    "        return examples, tally, raw_tally\n",
    "\n",
    "    def _do_train_job(self, sentences, alpha, inits):\n",
    "        \"\"\"Train the model on a single batch of sentences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        sentences : iterable of list of str\n",
    "            Corpus chunk to be used in this training batch.\n",
    "        alpha : float\n",
    "            The learning rate used in this batch.\n",
    "        inits : (np.ndarray, np.ndarray)\n",
    "            Each worker threads private work memory.\n",
    "        Returns\n",
    "        -------\n",
    "        (int, int)\n",
    "             2-tuple (effective word count after ignoring unknown words and sentence length trimming, total word count).\n",
    "        \"\"\"\n",
    "        work, neu1 = inits\n",
    "        tally = 0\n",
    "        if self.sg:\n",
    "            tally += train_batch_sg(self, sentences, alpha, work, self.compute_loss)\n",
    "        else:\n",
    "            tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
    "        return tally, self._raw_word_count(sentences)\n",
    "\n",
    "    def _clear_post_train(self):\n",
    "        \"\"\"Remove all L2-normalized word vectors from the model.\"\"\"\n",
    "        self.wv.vectors_norm = None\n",
    "\n",
    "    def _set_train_params(self, **kwargs):\n",
    "        if 'compute_loss' in kwargs:\n",
    "            self.compute_loss = kwargs['compute_loss']\n",
    "        self.running_training_loss = 0\n",
    "\n",
    "    def train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None,\n",
    "              epochs=None, start_alpha=None, end_alpha=None, word_count=0,\n",
    "              queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=()):\n",
    "        \"\"\"Update the model's neural weights from a sequence of sentences.\n",
    "        Notes\n",
    "        -----\n",
    "        To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
    "        progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
    "        raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
    "        that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
    "        you can simply use `total_examples=self.corpus_count`.\n",
    "        Warnings\n",
    "        --------\n",
    "        To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
    "        explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
    "        where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.iter`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        sentences : iterable of list of str\n",
    "            The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
    "            consider an iterable that streams the sentences directly from disk/network.\n",
    "            See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
    "            or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
    "            See also the `tutorial on data streaming in Python\n",
    "            <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
    "        corpus_file : str, optional\n",
    "            Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
    "            You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
    "            `corpus_file` arguments need to be passed (not both of them).\n",
    "        total_examples : int\n",
    "            Count of sentences.\n",
    "        total_words : int\n",
    "            Count of raw words in sentences.\n",
    "        epochs : int\n",
    "            Number of iterations (epochs) over the corpus.\n",
    "        start_alpha : float, optional\n",
    "            Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
    "            for this one call to`train()`.\n",
    "            Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
    "            (not recommended).\n",
    "        end_alpha : float, optional\n",
    "            Final learning rate. Drops linearly from `start_alpha`.\n",
    "            If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
    "            Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
    "            (not recommended).\n",
    "        word_count : int, optional\n",
    "            Count of words already trained. Set this to 0 for the usual\n",
    "            case of training on all words in sentences.\n",
    "        queue_factor : int, optional\n",
    "            Multiplier for size of queue (number of workers * queue_factor).\n",
    "        report_delay : float, optional\n",
    "            Seconds to wait before reporting progress.\n",
    "        compute_loss: bool, optional\n",
    "            If True, computes and stores loss value which can be retrieved using\n",
    "            :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
    "        callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
    "            Sequence of callbacks to be executed at specific stages during training.\n",
    "        Examples\n",
    "        --------\n",
    "        .. sourcecode:: pycon\n",
    "            >>> from gensim.models import Word2Vec\n",
    "            >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "            >>>\n",
    "            >>> model = Word2Vec(min_count=1)\n",
    "            >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
    "            >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors\n",
    "            (1, 30)\n",
    "        \"\"\"\n",
    "        return super(Word2Vec, self).train(\n",
    "            sentences=sentences, corpus_file=corpus_file, total_examples=total_examples, total_words=total_words,\n",
    "            epochs=epochs, start_alpha=start_alpha, end_alpha=end_alpha, word_count=word_count,\n",
    "            queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n",
    "\n",
    "    def score(self, sentences, total_sentences=int(1e6), chunksize=100, queue_factor=2, report_delay=1):\n",
    "        \"\"\"Score the log probability for a sequence of sentences.\n",
    "        This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
    "        Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
    "        so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
    "        Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
    "        score more than this number of sentences but it is inefficient to set the value too high.\n",
    "        See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
    "        <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
    "        `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
    "        how to use such scores in document classification.\n",
    "        Parameters\n",
    "        ----------\n",
    "        sentences : iterable of list of str\n",
    "            The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
    "            consider an iterable that streams the sentences directly from disk/network.\n",
    "            See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
    "            or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
    "        total_sentences : int, optional\n",
    "            Count of sentences.\n",
    "        chunksize : int, optional\n",
    "            Chunksize of jobs\n",
    "        queue_factor : int, optional\n",
    "            Multiplier for size of queue (number of workers * queue_factor).\n",
    "        report_delay : float, optional\n",
    "            Seconds to wait before reporting progress.\n",
    "        \"\"\"\n",
    "        if FAST_VERSION < 0:\n",
    "            warnings.warn(\n",
    "                \"C extension compilation failed, scoring will be slow. \"\n",
    "                \"Install a C compiler and reinstall gensim for fastness.\"\n",
    "            )\n",
    "\n",
    "        logger.info(\n",
    "            \"scoring sentences with %i workers on %i vocabulary and %i features, \"\n",
    "            \"using sg=%s hs=%s sample=%s and negative=%s\",\n",
    "            self.workers, len(self.wv.vocab), self.trainables.layer1_size, self.sg, self.hs,\n",
    "            self.vocabulary.sample, self.negative\n",
    "        )\n",
    "\n",
    "        if not self.wv.vocab:\n",
    "            raise RuntimeError(\"you must first build vocabulary before scoring new data\")\n",
    "\n",
    "        if not self.hs:\n",
    "            raise RuntimeError(\n",
    "                \"We have currently only implemented score for the hierarchical softmax scheme, \"\n",
    "                \"so you need to have run word2vec with hs=1 and negative=0 for this to work.\"\n",
    "            )\n",
    "\n",
    "        def worker_loop():\n",
    "            \"\"\"Compute log probability for each sentence, lifting lists of sentences from the jobs queue.\"\"\"\n",
    "            work = zeros(1, dtype=REAL)  # for sg hs, we actually only need one memory loc (running sum)\n",
    "            neu1 = matutils.zeros_aligned(self.trainables.layer1_size, dtype=REAL)\n",
    "            while True:\n",
    "                job = job_queue.get()\n",
    "                if job is None:  # signal to finish\n",
    "                    break\n",
    "                ns = 0\n",
    "                for sentence_id, sentence in job:\n",
    "                    if sentence_id >= total_sentences:\n",
    "                        break\n",
    "                    if self.sg:\n",
    "                        score = score_sentence_sg(self, sentence, work)\n",
    "                    else:\n",
    "                        score = score_sentence_cbow(self, sentence, work, neu1)\n",
    "                    sentence_scores[sentence_id] = score\n",
    "                    ns += 1\n",
    "                progress_queue.put(ns)  # report progress\n",
    "\n",
    "        start, next_report = default_timer(), 1.0\n",
    "        # buffer ahead only a limited number of jobs.. this is the reason we can't simply use ThreadPool :(\n",
    "        job_queue = Queue(maxsize=queue_factor * self.workers)\n",
    "        progress_queue = Queue(maxsize=(queue_factor + 1) * self.workers)\n",
    "\n",
    "        workers = [threading.Thread(target=worker_loop) for _ in range(self.workers)]\n",
    "        for thread in workers:\n",
    "            thread.daemon = True  # make interrupting the process with ctrl+c easier\n",
    "            thread.start()\n",
    "\n",
    "        sentence_count = 0\n",
    "        sentence_scores = matutils.zeros_aligned(total_sentences, dtype=REAL)\n",
    "\n",
    "        push_done = False\n",
    "        done_jobs = 0\n",
    "        jobs_source = enumerate(utils.grouper(enumerate(sentences), chunksize))\n",
    "\n",
    "        # fill jobs queue with (id, sentence) job items\n",
    "        while True:\n",
    "            try:\n",
    "                job_no, items = next(jobs_source)\n",
    "                if (job_no - 1) * chunksize > total_sentences:\n",
    "                    logger.warning(\n",
    "                        \"terminating after %i sentences (set higher total_sentences if you want more).\",\n",
    "                        total_sentences\n",
    "                    )\n",
    "                    job_no -= 1\n",
    "                    raise StopIteration()\n",
    "                logger.debug(\"putting job #%i in the queue\", job_no)\n",
    "                job_queue.put(items)\n",
    "            except StopIteration:\n",
    "                logger.info(\"reached end of input; waiting to finish %i outstanding jobs\", job_no - done_jobs + 1)\n",
    "                for _ in range(self.workers):\n",
    "                    job_queue.put(None)  # give the workers heads up that they can finish -- no more work!\n",
    "                push_done = True\n",
    "            try:\n",
    "                while done_jobs < (job_no + 1) or not push_done:\n",
    "                    ns = progress_queue.get(push_done)  # only block after all jobs pushed\n",
    "                    sentence_count += ns\n",
    "                    done_jobs += 1\n",
    "                    elapsed = default_timer() - start\n",
    "                    if elapsed >= next_report:\n",
    "                        logger.info(\n",
    "                            \"PROGRESS: at %.2f%% sentences, %.0f sentences/s\",\n",
    "                            100.0 * sentence_count, sentence_count / elapsed\n",
    "                        )\n",
    "                        next_report = elapsed + report_delay  # don't flood log, wait report_delay seconds\n",
    "                else:\n",
    "                    # loop ended by job count; really done\n",
    "                    break\n",
    "            except Empty:\n",
    "                pass  # already out of loop; continue to next push\n",
    "\n",
    "        elapsed = default_timer() - start\n",
    "        self.clear_sims()\n",
    "        logger.info(\n",
    "            \"scoring %i sentences took %.1fs, %.0f sentences/s\",\n",
    "            sentence_count, elapsed, sentence_count / elapsed\n",
    "        )\n",
    "        return sentence_scores[:sentence_count]\n",
    "\n",
    "    def clear_sims(self):\n",
    "        \"\"\"Remove all L2-normalized word vectors from the model, to free up memory.\n",
    "        You can recompute them later again using the :meth:`~gensim.models.word2vec.Word2Vec.init_sims` method.\n",
    "        \"\"\"\n",
    "        self.wv.vectors_norm = None\n",
    "\n",
    "    def intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict'):\n",
    "        \"\"\"Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
    "        where it intersects with the current vocabulary.\n",
    "        No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
    "        non-intersecting words are left alone.\n",
    "        Parameters\n",
    "        ----------\n",
    "        fname : str\n",
    "            The file path to load the vectors from.\n",
    "        lockf : float, optional\n",
    "            Lock-factor value to be set for any imported word-vectors; the\n",
    "            default value of 0.0 prevents further updating of the vector during subsequent\n",
    "            training. Use 1.0 to allow further training updates of merged vectors.\n",
    "        binary : bool, optional\n",
    "            If True, `fname` is in the binary word2vec C format.\n",
    "        encoding : str, optional\n",
    "            Encoding of `text` for `unicode` function (python2 only).\n",
    "        unicode_errors : str, optional\n",
    "            Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
    "        \"\"\"\n",
    "        overlap_count = 0\n",
    "        logger.info(\"loading projection weights from %s\", fname)\n",
    "        with utils.open(fname, 'rb') as fin:\n",
    "            header = utils.to_unicode(fin.readline(), encoding=encoding)\n",
    "            vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format\n",
    "            if not vector_size == self.wv.vector_size:\n",
    "                raise ValueError(\"incompatible vector size %d in file %s\" % (vector_size, fname))\n",
    "                # TOCONSIDER: maybe mismatched vectors still useful enough to merge (truncating/padding)?\n",
    "            if binary:\n",
    "                binary_len = dtype(REAL).itemsize * vector_size\n",
    "                for _ in range(vocab_size):\n",
    "                    # mixed text and binary: read text first, then binary\n",
    "                    word = []\n",
    "                    while True:\n",
    "                        ch = fin.read(1)\n",
    "                        if ch == b' ':\n",
    "                            break\n",
    "                        if ch != b'\\n':  # ignore newlines in front of words (some binary files have)\n",
    "                            word.append(ch)\n",
    "                    word = utils.to_unicode(b''.join(word), encoding=encoding, errors=unicode_errors)\n",
    "                    weights = fromstring(fin.read(binary_len), dtype=REAL)\n",
    "                    if word in self.wv.vocab:\n",
    "                        overlap_count += 1\n",
    "                        self.wv.vectors[self.wv.vocab[word].index] = weights\n",
    "                        self.trainables.vectors_lockf[self.wv.vocab[word].index] = lockf  # lock-factor: 0.0=no changes\n",
    "            else:\n",
    "                for line_no, line in enumerate(fin):\n",
    "                    parts = utils.to_unicode(line.rstrip(), encoding=encoding, errors=unicode_errors).split(\" \")\n",
    "                    if len(parts) != vector_size + 1:\n",
    "                        raise ValueError(\"invalid vector on line %s (is this really the text format?)\" % line_no)\n",
    "                    word, weights = parts[0], [REAL(x) for x in parts[1:]]\n",
    "                    if word in self.wv.vocab:\n",
    "                        overlap_count += 1\n",
    "                        self.wv.vectors[self.wv.vocab[word].index] = weights\n",
    "                        self.trainables.vectors_lockf[self.wv.vocab[word].index] = lockf  # lock-factor: 0.0=no changes\n",
    "        logger.info(\"merged %d vectors into %s matrix from %s\", overlap_count, self.wv.vectors.shape, fname)\n",
    "\n",
    "    @deprecated(\"Method will be removed in 4.0.0, use self.wv.__getitem__() instead\")\n",
    "    def __getitem__(self, words):\n",
    "        \"\"\"Deprecated. Use `self.wv.__getitem__` instead.\n",
    "        Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__getitem__`.\n",
    "        \"\"\"\n",
    "        return self.wv.__getitem__(words)\n",
    "\n",
    "    @deprecated(\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\")\n",
    "    def __contains__(self, word):\n",
    "        \"\"\"Deprecated. Use `self.wv.__contains__` instead.\n",
    "        Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__contains__`.\n",
    "        \"\"\"\n",
    "        return self.wv.__contains__(word)\n",
    "\n",
    "    def predict_output_word(self, context_words_list, topn=10):\n",
    "        \"\"\"Get the probability distribution of the center word given context words.\n",
    "        Parameters\n",
    "        ----------\n",
    "        context_words_list : list of str\n",
    "            List of context words.\n",
    "        topn : int, optional\n",
    "            Return `topn` words and their probabilities.\n",
    "        Returns\n",
    "        -------\n",
    "        list of (str, float)\n",
    "            `topn` length list of tuples of (word, probability).\n",
    "        \"\"\"\n",
    "        if not self.negative:\n",
    "            raise RuntimeError(\n",
    "                \"We have currently only implemented predict_output_word for the negative sampling scheme, \"\n",
    "                \"so you need to have run word2vec with negative > 0 for this to work.\"\n",
    "            )\n",
    "\n",
    "        if not hasattr(self.wv, 'vectors') or not hasattr(self.trainables, 'syn1neg'):\n",
    "            raise RuntimeError(\"Parameters required for predicting the output words not found.\")\n",
    "\n",
    "        word_vocabs = [self.wv.vocab[w] for w in context_words_list if w in self.wv.vocab]\n",
    "        if not word_vocabs:\n",
    "            warnings.warn(\"All the input context words are out-of-vocabulary for the current model.\")\n",
    "            return None\n",
    "\n",
    "        word2_indices = [word.index for word in word_vocabs]\n",
    "\n",
    "        l1 = np_sum(self.wv.vectors[word2_indices], axis=0)\n",
    "        if word2_indices and self.cbow_mean:\n",
    "            l1 /= len(word2_indices)\n",
    "\n",
    "        # propagate hidden -> output and take softmax to get probabilities\n",
    "        prob_values = exp(dot(l1, self.trainables.syn1neg.T))\n",
    "        prob_values /= sum(prob_values)\n",
    "        top_indices = matutils.argsort(prob_values, topn=topn, reverse=True)\n",
    "        # returning the most probable output words with their probabilities\n",
    "        return [(self.wv.index2word[index1], prob_values[index1]) for index1 in top_indices]\n",
    "\n",
    "    def init_sims(self, replace=False):\n",
    "        \"\"\"Deprecated. Use `self.wv.init_sims` instead.\n",
    "        See :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.init_sims`.\n",
    "        \"\"\"\n",
    "        if replace and hasattr(self.trainables, 'syn1'):\n",
    "            del self.trainables.syn1\n",
    "        return self.wv.init_sims(replace)\n",
    "\n",
    "    def reset_from(self, other_model):\n",
    "        \"\"\"Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
    "        Structures copied are:\n",
    "            * Vocabulary\n",
    "            * Index to word mapping\n",
    "            * Cumulative frequency table (used for negative sampling)\n",
    "            * Cached corpus length\n",
    "        Useful when testing multiple models on the same corpus in parallel.\n",
    "        Parameters\n",
    "        ----------\n",
    "        other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
    "            Another model to copy the internal structures from.\n",
    "        \"\"\"\n",
    "        self.wv.vocab = other_model.wv.vocab\n",
    "        self.wv.index2word = other_model.wv.index2word\n",
    "        self.vocabulary.cum_table = other_model.vocabulary.cum_table\n",
    "        self.corpus_count = other_model.corpus_count\n",
    "        self.trainables.reset_weights(self.hs, self.negative, self.wv)\n",
    "\n",
    "    @staticmethod\n",
    "    def log_accuracy(section):\n",
    "        \"\"\"Deprecated. Use `self.wv.log_accuracy` instead.\n",
    "        See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.log_accuracy`.\n",
    "        \"\"\"\n",
    "        return Word2VecKeyedVectors.log_accuracy(section)\n",
    "\n",
    "    @deprecated(\"Method will be removed in 4.0.0, use self.wv.evaluate_word_analogies() instead\")\n",
    "    def accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True):\n",
    "        \"\"\"Deprecated. Use `self.wv.accuracy` instead.\n",
    "        See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.accuracy`.\n",
    "        \"\"\"\n",
    "        most_similar = most_similar or Word2VecKeyedVectors.most_similar\n",
    "        return self.wv.accuracy(questions, restrict_vocab, most_similar, case_insensitive)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Human readable representation of the model's state.\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Human readable representation of the model's state, including the vocabulary size, vector size\n",
    "            and learning rate.\n",
    "        \"\"\"\n",
    "        return \"%s(vocab=%s, size=%s, alpha=%s)\" % (\n",
    "            self.__class__.__name__, len(self.wv.index2word), self.wv.vector_size, self.alpha\n",
    "        )\n",
    "\n",
    "    def delete_temporary_training_data(self, replace_word_vectors_with_normalized=False):\n",
    "        \"\"\"Discard parameters that are used in training and scoring, to save memory.\n",
    "        Warnings\n",
    "        --------\n",
    "        Use only if you're sure you're done training a model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        replace_word_vectors_with_normalized : bool, optional\n",
    "            If True, forget the original (not normalized) word vectors and only keep\n",
    "            the L2-normalized word vectors, to save even more memory.\n",
    "        \"\"\"\n",
    "        if replace_word_vectors_with_normalized:\n",
    "            self.init_sims(replace=True)\n",
    "        self._minimize_model()\n",
    "\n",
    "    def save(self, *args, **kwargs):\n",
    "        \"\"\"Save the model.\n",
    "        This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
    "        online training and getting vectors for vocabulary words.\n",
    "        Parameters\n",
    "        ----------\n",
    "        fname : str\n",
    "            Path to the file.\n",
    "        \"\"\"\n",
    "        # don't bother storing the cached normalized vectors, recalculable table\n",
    "        kwargs['ignore'] = kwargs.get('ignore', ['vectors_norm', 'cum_table'])\n",
    "        super(Word2Vec, self).save(*args, **kwargs)\n",
    "\n",
    "    def get_latest_training_loss(self):\n",
    "        \"\"\"Get current value of the training loss.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Current training loss.\n",
    "        \"\"\"\n",
    "        return self.running_training_loss\n",
    "\n",
    "    @deprecated(\n",
    "        \"Method will be removed in 4.0.0, keep just_word_vectors = model.wv to retain just the KeyedVectors instance\"\n",
    "    )\n",
    "    def _minimize_model(self, save_syn1=False, save_syn1neg=False, save_vectors_lockf=False):\n",
    "        if save_syn1 and save_syn1neg and save_vectors_lockf:\n",
    "            return\n",
    "        if hasattr(self.trainables, 'syn1') and not save_syn1:\n",
    "            del self.trainables.syn1\n",
    "        if hasattr(self.trainables, 'syn1neg') and not save_syn1neg:\n",
    "            del self.trainables.syn1neg\n",
    "        if hasattr(self.trainables, 'vectors_lockf') and not save_vectors_lockf:\n",
    "            del self.trainables.vectors_lockf\n",
    "        self.model_trimmed_post_training = True\n",
    "\n",
    "    @classmethod\n",
    "    def load_word2vec_format(\n",
    "            cls, fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict',\n",
    "            limit=None, datatype=REAL):\n",
    "        \"\"\"Deprecated. Use :meth:`gensim.models.KeyedVectors.load_word2vec_format` instead.\"\"\"\n",
    "        raise DeprecationWarning(\"Deprecated. Use gensim.models.KeyedVectors.load_word2vec_format instead.\")\n",
    "\n",
    "    def save_word2vec_format(self, fname, fvocab=None, binary=False):\n",
    "        \"\"\"Deprecated. Use `model.wv.save_word2vec_format` instead.\n",
    "        See :meth:`gensim.models.KeyedVectors.save_word2vec_format`.\n",
    "        \"\"\"\n",
    "        raise DeprecationWarning(\"Deprecated. Use model.wv.save_word2vec_format instead.\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, *args, **kwargs):\n",
    "        \"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
    "        See Also\n",
    "        --------\n",
    "        :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
    "            Save model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        fname : str\n",
    "            Path to the saved file.\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`~gensim.models.word2vec.Word2Vec`\n",
    "            Loaded model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            model = super(Word2Vec, cls).load(*args, **kwargs)\n",
    "\n",
    "            # for backward compatibility for `max_final_vocab` feature\n",
    "            if not hasattr(model, 'max_final_vocab'):\n",
    "                model.max_final_vocab = None\n",
    "                model.vocabulary.max_final_vocab = None\n",
    "\n",
    "            return model\n",
    "        except AttributeError:\n",
    "            logger.info('Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.')\n",
    "            from gensim.models.deprecated.word2vec import load_old_word2vec\n",
    "            return load_old_word2vec(*args, **kwargs)\n",
    "\n",
    "\n",
    "class BrownCorpus(object):\n",
    "    \"\"\"Iterate over sentences from the `Brown corpus <https://en.wikipedia.org/wiki/Brown_Corpus>`_\n",
    "     (part of `NLTK data <https://www.nltk.org/data.html>`_).\n",
    "    \"\"\"\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            fname = os.path.join(self.dirname, fname)\n",
    "            if not os.path.isfile(fname):\n",
    "                continue\n",
    "            with utils.open(fname, 'rb') as fin:\n",
    "                for line in fin:\n",
    "                    line = utils.to_unicode(line)\n",
    "                    # each file line is a single sentence in the Brown corpus\n",
    "                    # each token is WORD/POS_TAG\n",
    "                    token_tags = [t.split('/') for t in line.split() if len(t.split('/')) == 2]\n",
    "                    # ignore words with non-alphabetic tags like \",\", \"!\" etc (punctuation, weird stuff)\n",
    "                    words = [\"%s/%s\" % (token.lower(), tag[:2]) for token, tag in token_tags if tag[:2].isalpha()]\n",
    "                    if not words:  # don't bother sending out empty sentences\n",
    "                        continue\n",
    "                    yield words\n",
    "\n",
    "\n",
    "class Text8Corpus(object):\n",
    "    \"\"\"Iterate over sentences from the \"text8\" corpus, unzipped from http://mattmahoney.net/dc/text8.zip.\"\"\"\n",
    "    def __init__(self, fname, max_sentence_length=MAX_WORDS_IN_BATCH):\n",
    "        self.fname = fname\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        # the entire corpus is one gigantic line -- there are no sentence marks at all\n",
    "        # so just split the sequence of tokens arbitrarily: 1 sentence = 1000 tokens\n",
    "        sentence, rest = [], b''\n",
    "        with utils.open(self.fname, 'rb') as fin:\n",
    "            while True:\n",
    "                text = rest + fin.read(8192)  # avoid loading the entire file (=1 line) into RAM\n",
    "                if text == rest:  # EOF\n",
    "                    words = utils.to_unicode(text).split()\n",
    "                    sentence.extend(words)  # return the last chunk of words, too (may be shorter/longer)\n",
    "                    if sentence:\n",
    "                        yield sentence\n",
    "                    break\n",
    "                last_token = text.rfind(b' ')  # last token may have been split in two... keep for next iteration\n",
    "                words, rest = (utils.to_unicode(text[:last_token]).split(),\n",
    "                               text[last_token:].strip()) if last_token >= 0 else ([], text)\n",
    "                sentence.extend(words)\n",
    "                while len(sentence) >= self.max_sentence_length:\n",
    "                    yield sentence[:self.max_sentence_length]\n",
    "                    sentence = sentence[self.max_sentence_length:]\n",
    "\n",
    "\n",
    "class LineSentence(object):\n",
    "    \"\"\"Iterate over a file that contains sentences: one line = one sentence.\n",
    "    Words must be already preprocessed and separated by whitespace.\n",
    "    \"\"\"\n",
    "    def __init__(self, source, max_sentence_length=MAX_WORDS_IN_BATCH, limit=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        source : string or a file-like object\n",
    "            Path to the file on disk, or an already-open file object (must support `seek(0)`).\n",
    "        limit : int or None\n",
    "            Clip the file to the first `limit` lines. Do no clipping if `limit is None` (the default).\n",
    "        Examples\n",
    "        --------\n",
    "        .. sourcecode:: pycon\n",
    "            >>> from gensim.test.utils import datapath\n",
    "            >>> sentences = LineSentence(datapath('lee_background.cor'))\n",
    "            >>> for sentence in sentences:\n",
    "            ...     pass\n",
    "        \"\"\"\n",
    "        self.source = source\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.limit = limit\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate through the lines in the source.\"\"\"\n",
    "        try:\n",
    "            # Assume it is a file-like object and try treating it as such\n",
    "            # Things that don't have seek will trigger an exception\n",
    "            self.source.seek(0)\n",
    "            for line in itertools.islice(self.source, self.limit):\n",
    "                line = utils.to_unicode(line).split()\n",
    "                i = 0\n",
    "                while i < len(line):\n",
    "                    yield line[i: i + self.max_sentence_length]\n",
    "                    i += self.max_sentence_length\n",
    "        except AttributeError:\n",
    "            # If it didn't work like a file, use it as a string filename\n",
    "            with utils.open(self.source, 'rb') as fin:\n",
    "                for line in itertools.islice(fin, self.limit):\n",
    "                    line = utils.to_unicode(line).split()\n",
    "                    i = 0\n",
    "                    while i < len(line):\n",
    "                        yield line[i: i + self.max_sentence_length]\n",
    "                        i += self.max_sentence_length\n",
    "\n",
    "\n",
    "class PathLineSentences(object):\n",
    "    \"\"\"Like :class:`~gensim.models.word2vec.LineSentence`, but process all files in a directory\n",
    "    in alphabetical order by filename.\n",
    "    The directory must only contain files that can be read by :class:`gensim.models.word2vec.LineSentence`:\n",
    "    .bz2, .gz, and text files. Any file not ending with .bz2 or .gz is assumed to be a text file.\n",
    "    The format of files (either text, or compressed text files) in the path is one sentence = one line,\n",
    "    with words already preprocessed and separated by whitespace.\n",
    "    Warnings\n",
    "    --------\n",
    "    Does **not recurse** into subdirectories.\n",
    "    \"\"\"\n",
    "    def __init__(self, source, max_sentence_length=MAX_WORDS_IN_BATCH, limit=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        source : str\n",
    "            Path to the directory.\n",
    "        limit : int or None\n",
    "            Read only the first `limit` lines from each file. Read all if limit is None (the default).\n",
    "        \"\"\"\n",
    "        self.source = source\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.limit = limit\n",
    "\n",
    "        if os.path.isfile(self.source):\n",
    "            logger.debug('single file given as source, rather than a directory of files')\n",
    "            logger.debug('consider using models.word2vec.LineSentence for a single file')\n",
    "            self.input_files = [self.source]  # force code compatibility with list of files\n",
    "        elif os.path.isdir(self.source):\n",
    "            self.source = os.path.join(self.source, '')  # ensures os-specific slash at end of path\n",
    "            logger.info('reading directory %s', self.source)\n",
    "            self.input_files = os.listdir(self.source)\n",
    "            self.input_files = [self.source + filename for filename in self.input_files]  # make full paths\n",
    "            self.input_files.sort()  # makes sure it happens in filename order\n",
    "        else:  # not a file or a directory, then we can't do anything with it\n",
    "            raise ValueError('input is neither a file nor a path')\n",
    "        logger.info('files read into PathLineSentences:%s', '\\n'.join(self.input_files))\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"iterate through the files\"\"\"\n",
    "        for file_name in self.input_files:\n",
    "            logger.info('reading file %s', file_name)\n",
    "            with utils.open(file_name, 'rb') as fin:\n",
    "                for line in itertools.islice(fin, self.limit):\n",
    "                    line = utils.to_unicode(line).split()\n",
    "                    i = 0\n",
    "                    while i < len(line):\n",
    "                        yield line[i:i + self.max_sentence_length]\n",
    "                        i += self.max_sentence_length\n",
    "\n",
    "\n",
    "def _scan_vocab_worker(stream, progress_queue, max_vocab_size=None, trim_rule=None):\n",
    "    \"\"\"Do an initial scan of all words appearing in stream.\n",
    "    Note: This function can not be Word2VecVocab's method because\n",
    "    of multiprocessing synchronization specifics in Python.\n",
    "    \"\"\"\n",
    "    min_reduce = 1\n",
    "    vocab = defaultdict(int)\n",
    "    checked_string_types = 0\n",
    "    sentence_no = -1\n",
    "    total_words = 0\n",
    "    for sentence_no, sentence in enumerate(stream):\n",
    "        if not checked_string_types:\n",
    "            if isinstance(sentence, string_types):\n",
    "                log_msg = \"Each 'sentences' item should be a list of words (usually unicode strings). \" \\\n",
    "                          \"First item here is instead plain %s.\" % type(sentence)\n",
    "                progress_queue.put(log_msg)\n",
    "\n",
    "            checked_string_types += 1\n",
    "\n",
    "        for word in sentence:\n",
    "            vocab[word] += 1\n",
    "\n",
    "        if max_vocab_size and len(vocab) > max_vocab_size:\n",
    "            utils.prune_vocab(vocab, min_reduce, trim_rule=trim_rule)\n",
    "            min_reduce += 1\n",
    "\n",
    "        total_words += len(sentence)\n",
    "\n",
    "    progress_queue.put((total_words, sentence_no + 1))\n",
    "    progress_queue.put(None)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "class Word2VecVocab(utils.SaveLoad):\n",
    "    \"\"\"Vocabulary used by :class:`~gensim.models.word2vec.Word2Vec`.\"\"\"\n",
    "    def __init__(\n",
    "            self, max_vocab_size=None, min_count=5, sample=1e-3, sorted_vocab=True, null_word=0,\n",
    "            max_final_vocab=None, ns_exponent=0.75):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_count = min_count\n",
    "        self.sample = sample\n",
    "        self.sorted_vocab = sorted_vocab\n",
    "        self.null_word = null_word\n",
    "        self.cum_table = None  # for negative sampling\n",
    "        self.raw_vocab = None\n",
    "        self.max_final_vocab = max_final_vocab\n",
    "        self.ns_exponent = ns_exponent\n",
    "\n",
    "    def _scan_vocab(self, sentences, progress_per, trim_rule):\n",
    "        sentence_no = -1\n",
    "        total_words = 0\n",
    "        min_reduce = 1\n",
    "        vocab = defaultdict(int)\n",
    "        checked_string_types = 0\n",
    "        for sentence_no, sentence in enumerate(sentences):\n",
    "            if not checked_string_types:\n",
    "                if isinstance(sentence, string_types):\n",
    "                    logger.warning(\n",
    "                        \"Each 'sentences' item should be a list of words (usually unicode strings). \"\n",
    "                        \"First item here is instead plain %s.\",\n",
    "                        type(sentence)\n",
    "                    )\n",
    "                checked_string_types += 1\n",
    "            if sentence_no % progress_per == 0:\n",
    "                logger.info(\n",
    "                    \"PROGRESS: at sentence #%i, processed %i words, keeping %i word types\",\n",
    "                    sentence_no, total_words, len(vocab)\n",
    "                )\n",
    "            for word in sentence:\n",
    "                vocab[word] += 1\n",
    "            total_words += len(sentence)\n",
    "\n",
    "            if self.max_vocab_size and len(vocab) > self.max_vocab_size:\n",
    "                utils.prune_vocab(vocab, min_reduce, trim_rule=trim_rule)\n",
    "                min_reduce += 1\n",
    "\n",
    "        corpus_count = sentence_no + 1\n",
    "        self.raw_vocab = vocab\n",
    "        return total_words, corpus_count\n",
    "\n",
    "    def scan_vocab(self, sentences=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None):\n",
    "        logger.info(\"collecting all words and their counts\")\n",
    "        if corpus_file:\n",
    "            sentences = LineSentence(corpus_file)\n",
    "\n",
    "        total_words, corpus_count = self._scan_vocab(sentences, progress_per, trim_rule)\n",
    "\n",
    "        logger.info(\n",
    "            \"collected %i word types from a corpus of %i raw words and %i sentences\",\n",
    "            len(self.raw_vocab), total_words, corpus_count\n",
    "        )\n",
    "\n",
    "        return total_words, corpus_count\n",
    "\n",
    "    def sort_vocab(self, wv):\n",
    "        \"\"\"Sort the vocabulary so the most frequent words have the lowest indexes.\"\"\"\n",
    "        if len(wv.vectors):\n",
    "            raise RuntimeError(\"cannot sort vocabulary after model weights already initialized.\")\n",
    "        wv.index2word.sort(key=lambda word: wv.vocab[word].count, reverse=True)\n",
    "        for i, word in enumerate(wv.index2word):\n",
    "            wv.vocab[word].index = i\n",
    "\n",
    "    def prepare_vocab(\n",
    "            self, hs, negative, wv, update=False, keep_raw_vocab=False, trim_rule=None,\n",
    "            min_count=None, sample=None, dry_run=False):\n",
    "        \"\"\"Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
    "        and `sample` (controlling the downsampling of more-frequent words).\n",
    "        Calling with `dry_run=True` will only simulate the provided settings and\n",
    "        report the size of the retained vocabulary, effective corpus length, and\n",
    "        estimated memory requirements. Results are both printed via logging and\n",
    "        returned as a dict.\n",
    "        Delete the raw vocabulary after the scaling is done to free up RAM,\n",
    "        unless `keep_raw_vocab` is set.\n",
    "        \"\"\"\n",
    "        min_count = min_count or self.min_count\n",
    "        sample = sample or self.sample\n",
    "        drop_total = drop_unique = 0\n",
    "\n",
    "        # set effective_min_count to min_count in case max_final_vocab isn't set\n",
    "        self.effective_min_count = min_count\n",
    "\n",
    "        # if max_final_vocab is specified instead of min_count\n",
    "        # pick a min_count which satisfies max_final_vocab as well as possible\n",
    "        if self.max_final_vocab is not None:\n",
    "            sorted_vocab = sorted(self.raw_vocab.keys(), key=lambda word: self.raw_vocab[word], reverse=True)\n",
    "            calc_min_count = 1\n",
    "\n",
    "            if self.max_final_vocab < len(sorted_vocab):\n",
    "                calc_min_count = self.raw_vocab[sorted_vocab[self.max_final_vocab]] + 1\n",
    "\n",
    "            self.effective_min_count = max(calc_min_count, min_count)\n",
    "            logger.info(\n",
    "                \"max_final_vocab=%d and min_count=%d resulted in calc_min_count=%d, effective_min_count=%d\",\n",
    "                self.max_final_vocab, min_count, calc_min_count, self.effective_min_count\n",
    "            )\n",
    "\n",
    "        if not update:\n",
    "            logger.info(\"Loading a fresh vocabulary\")\n",
    "            retain_total, retain_words = 0, []\n",
    "            # Discard words less-frequent than min_count\n",
    "            if not dry_run:\n",
    "                wv.index2word = []\n",
    "                # make stored settings match these applied settings\n",
    "                self.min_count = min_count\n",
    "                self.sample = sample\n",
    "                wv.vocab = {}\n",
    "\n",
    "            for word, v in iteritems(self.raw_vocab):\n",
    "                if keep_vocab_item(word, v, self.effective_min_count, trim_rule=trim_rule):\n",
    "                    retain_words.append(word)\n",
    "                    retain_total += v\n",
    "                    if not dry_run:\n",
    "                        wv.vocab[word] = Vocab(count=v, index=len(wv.index2word))\n",
    "                        wv.index2word.append(word)\n",
    "                else:\n",
    "                    drop_unique += 1\n",
    "                    drop_total += v\n",
    "            original_unique_total = len(retain_words) + drop_unique\n",
    "            retain_unique_pct = len(retain_words) * 100 / max(original_unique_total, 1)\n",
    "            logger.info(\n",
    "                \"effective_min_count=%d retains %i unique words (%i%% of original %i, drops %i)\",\n",
    "                self.effective_min_count, len(retain_words), retain_unique_pct, original_unique_total, drop_unique\n",
    "            )\n",
    "            original_total = retain_total + drop_total\n",
    "            retain_pct = retain_total * 100 / max(original_total, 1)\n",
    "            logger.info(\n",
    "                \"effective_min_count=%d leaves %i word corpus (%i%% of original %i, drops %i)\",\n",
    "                self.effective_min_count, retain_total, retain_pct, original_total, drop_total\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"Updating model with new vocabulary\")\n",
    "            new_total = pre_exist_total = 0\n",
    "            new_words = pre_exist_words = []\n",
    "            for word, v in iteritems(self.raw_vocab):\n",
    "                if keep_vocab_item(word, v, self.effective_min_count, trim_rule=trim_rule):\n",
    "                    if word in wv.vocab:\n",
    "                        pre_exist_words.append(word)\n",
    "                        pre_exist_total += v\n",
    "                        if not dry_run:\n",
    "                            wv.vocab[word].count += v\n",
    "                    else:\n",
    "                        new_words.append(word)\n",
    "                        new_total += v\n",
    "                        if not dry_run:\n",
    "                            wv.vocab[word] = Vocab(count=v, index=len(wv.index2word))\n",
    "                            wv.index2word.append(word)\n",
    "                else:\n",
    "                    drop_unique += 1\n",
    "                    drop_total += v\n",
    "            original_unique_total = len(pre_exist_words) + len(new_words) + drop_unique\n",
    "            pre_exist_unique_pct = len(pre_exist_words) * 100 / max(original_unique_total, 1)\n",
    "            new_unique_pct = len(new_words) * 100 / max(original_unique_total, 1)\n",
    "            logger.info(\n",
    "                \"New added %i unique words (%i%% of original %i) \"\n",
    "                \"and increased the count of %i pre-existing words (%i%% of original %i)\",\n",
    "                len(new_words), new_unique_pct, original_unique_total, len(pre_exist_words),\n",
    "                pre_exist_unique_pct, original_unique_total\n",
    "            )\n",
    "            retain_words = new_words + pre_exist_words\n",
    "            retain_total = new_total + pre_exist_total\n",
    "\n",
    "        # Precalculate each vocabulary item's threshold for sampling\n",
    "        if not sample:\n",
    "            # no words downsampled\n",
    "            threshold_count = retain_total\n",
    "        elif sample < 1.0:\n",
    "            # traditional meaning: set parameter as proportion of total\n",
    "            threshold_count = sample * retain_total\n",
    "        else:\n",
    "            # new shorthand: sample >= 1 means downsample all words with higher count than sample\n",
    "            threshold_count = int(sample * (3 + sqrt(5)) / 2)\n",
    "\n",
    "        downsample_total, downsample_unique = 0, 0\n",
    "        for w in retain_words:\n",
    "            v = self.raw_vocab[w]\n",
    "            word_probability = (sqrt(v / threshold_count) + 1) * (threshold_count / v)\n",
    "            if word_probability < 1.0:\n",
    "                downsample_unique += 1\n",
    "                downsample_total += word_probability * v\n",
    "            else:\n",
    "                word_probability = 1.0\n",
    "                downsample_total += v\n",
    "            if not dry_run:\n",
    "                wv.vocab[w].sample_int = int(round(word_probability * 2**32))\n",
    "\n",
    "        if not dry_run and not keep_raw_vocab:\n",
    "            logger.info(\"deleting the raw counts dictionary of %i items\", len(self.raw_vocab))\n",
    "            self.raw_vocab = defaultdict(int)\n",
    "\n",
    "        logger.info(\"sample=%g downsamples %i most-common words\", sample, downsample_unique)\n",
    "        logger.info(\n",
    "            \"downsampling leaves estimated %i word corpus (%.1f%% of prior %i)\",\n",
    "            downsample_total, downsample_total * 100.0 / max(retain_total, 1), retain_total\n",
    "        )\n",
    "\n",
    "        # return from each step: words-affected, resulting-corpus-size, extra memory estimates\n",
    "        report_values = {\n",
    "            'drop_unique': drop_unique, 'retain_total': retain_total, 'downsample_unique': downsample_unique,\n",
    "            'downsample_total': int(downsample_total), 'num_retained_words': len(retain_words)\n",
    "        }\n",
    "\n",
    "        if self.null_word:\n",
    "            # create null pseudo-word for padding when using concatenative L1 (run-of-words)\n",
    "            # this word is only ever input – never predicted – so count, huffman-point, etc doesn't matter\n",
    "            self.add_null_word(wv)\n",
    "\n",
    "        if self.sorted_vocab and not update:\n",
    "            self.sort_vocab(wv)\n",
    "        if hs:\n",
    "            # add info about each word's Huffman encoding\n",
    "            self.create_binary_tree(wv)\n",
    "        if negative:\n",
    "            # build the table for drawing random words (for negative sampling)\n",
    "            self.make_cum_table(wv)\n",
    "\n",
    "        return report_values\n",
    "\n",
    "    def add_null_word(self, wv):\n",
    "        word, v = '\\0', Vocab(count=1, sample_int=0)\n",
    "        v.index = len(wv.vocab)\n",
    "        wv.index2word.append(word)\n",
    "        wv.vocab[word] = v\n",
    "\n",
    "    def create_binary_tree(self, wv):\n",
    "        \"\"\"Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
    "        word counts. Frequent words will have shorter binary codes.\n",
    "        Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
    "        \"\"\"\n",
    "        _assign_binary_codes(wv.vocab)\n",
    "\n",
    "    def make_cum_table(self, wv, domain=2**31 - 1):\n",
    "        \"\"\"Create a cumulative-distribution table using stored vocabulary word counts for\n",
    "        drawing random words in the negative-sampling training routines.\n",
    "        To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
    "        then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
    "        That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
    "        Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
    "        \"\"\"\n",
    "        vocab_size = len(wv.index2word)\n",
    "        self.cum_table = zeros(vocab_size, dtype=uint32)\n",
    "        # compute sum of all power (Z in paper)\n",
    "        train_words_pow = 0.0\n",
    "        for word_index in range(vocab_size):\n",
    "            train_words_pow += wv.vocab[wv.index2word[word_index]].count**self.ns_exponent\n",
    "        cumulative = 0.0\n",
    "        for word_index in range(vocab_size):\n",
    "            cumulative += wv.vocab[wv.index2word[word_index]].count**self.ns_exponent\n",
    "            self.cum_table[word_index] = round(cumulative / train_words_pow * domain)\n",
    "        if len(self.cum_table) > 0:\n",
    "            assert self.cum_table[-1] == domain\n",
    "\n",
    "\n",
    "def _build_heap(vocab):\n",
    "    heap = list(itervalues(vocab))\n",
    "    heapq.heapify(heap)\n",
    "    for i in range(len(vocab) - 1):\n",
    "        min1, min2 = heapq.heappop(heap), heapq.heappop(heap)\n",
    "        heapq.heappush(\n",
    "            heap, Vocab(count=min1.count + min2.count, index=i + len(vocab), left=min1, right=min2)\n",
    "        )\n",
    "    return heap\n",
    "\n",
    "\n",
    "def _assign_binary_codes(vocab):\n",
    "    \"\"\"\n",
    "    Appends a binary code to each vocab term.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : dict\n",
    "        A dictionary of :class:`gensim.models.word2vec.Vocab` objects.\n",
    "    Notes\n",
    "    -----\n",
    "    Expects each term to have an .index attribute that contains the order in\n",
    "    which the term was added to the vocabulary.  E.g. term.index == 0 means the\n",
    "    term was added to the vocab first.\n",
    "    Sets the .code and .point attributes of each node.\n",
    "    Each code is a numpy.array containing 0s and 1s.\n",
    "    Each point is an integer.\n",
    "    \"\"\"\n",
    "    logger.info(\"constructing a huffman tree from %i words\", len(vocab))\n",
    "\n",
    "    heap = _build_heap(vocab)\n",
    "    if not heap:\n",
    "        #\n",
    "        # TODO: how can we end up with an empty heap?\n",
    "        #\n",
    "        logger.info(\"built huffman tree with maximum node depth 0\")\n",
    "        return\n",
    "\n",
    "    # recurse over the tree, assigning a binary code to each vocabulary word\n",
    "    max_depth = 0\n",
    "    stack = [(heap[0], [], [])]\n",
    "    while stack:\n",
    "        node, codes, points = stack.pop()\n",
    "        if node.index < len(vocab):\n",
    "            # leaf node => store its path from the root\n",
    "            node.code, node.point = codes, points\n",
    "            max_depth = max(len(codes), max_depth)\n",
    "        else:\n",
    "            # inner node => continue recursion\n",
    "            points = array(list(points) + [node.index - len(vocab)], dtype=uint32)\n",
    "            stack.append((node.left, array(list(codes) + [0], dtype=uint8), points))\n",
    "            stack.append((node.right, array(list(codes) + [1], dtype=uint8), points))\n",
    "\n",
    "    logger.info(\"built huffman tree with maximum node depth %i\", max_depth)\n",
    "\n",
    "\n",
    "class Word2VecTrainables(utils.SaveLoad):\n",
    "    \"\"\"Represents the inner shallow neural network used to train :class:`~gensim.models.word2vec.Word2Vec`.\"\"\"\n",
    "    def __init__(self, vector_size=100, seed=1, hashfxn=hash):\n",
    "        self.hashfxn = hashfxn\n",
    "        self.layer1_size = vector_size\n",
    "        self.seed = seed\n",
    "\n",
    "    def prepare_weights(self, hs, negative, wv, update=False, vocabulary=None):\n",
    "        \"\"\"Build tables and model weights based on final vocabulary settings.\"\"\"\n",
    "        # set initial input/projection and hidden weights\n",
    "        if not update:\n",
    "            self.reset_weights(hs, negative, wv)\n",
    "        else:\n",
    "            self.update_weights(hs, negative, wv)\n",
    "\n",
    "    def seeded_vector(self, seed_string, vector_size):\n",
    "        \"\"\"Get a random vector (but deterministic by seed_string).\"\"\"\n",
    "        # Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\n",
    "        once = random.RandomState(self.hashfxn(seed_string) & 0xffffffff)\n",
    "        return (once.rand(vector_size) - 0.5) / vector_size\n",
    "\n",
    "    def reset_weights(self, hs, negative, wv):\n",
    "        \"\"\"Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\"\"\"\n",
    "        logger.info(\"resetting layer weights\")\n",
    "        wv.vectors = empty((len(wv.vocab), wv.vector_size), dtype=REAL)\n",
    "        # randomize weights vector by vector, rather than materializing a huge random matrix in RAM at once\n",
    "        for i in range(len(wv.vocab)):\n",
    "            # construct deterministic seed from word AND seed argument\n",
    "            wv.vectors[i] = self.seeded_vector(wv.index2word[i] + str(self.seed), wv.vector_size)\n",
    "        if hs:\n",
    "            self.syn1 = zeros((len(wv.vocab), self.layer1_size), dtype=REAL)\n",
    "        if negative:\n",
    "            self.syn1neg = zeros((len(wv.vocab), self.layer1_size), dtype=REAL)\n",
    "        wv.vectors_norm = None\n",
    "\n",
    "        self.vectors_lockf = ones(len(wv.vocab), dtype=REAL)  # zeros suppress learning\n",
    "\n",
    "    def update_weights(self, hs, negative, wv):\n",
    "        \"\"\"Copy all the existing weights, and reset the weights for the newly added vocabulary.\"\"\"\n",
    "        logger.info(\"updating layer weights\")\n",
    "        gained_vocab = len(wv.vocab) - len(wv.vectors)\n",
    "        newvectors = empty((gained_vocab, wv.vector_size), dtype=REAL)\n",
    "\n",
    "        # randomize the remaining words\n",
    "        for i in range(len(wv.vectors), len(wv.vocab)):\n",
    "            # construct deterministic seed from word AND seed argument\n",
    "            newvectors[i - len(wv.vectors)] = self.seeded_vector(wv.index2word[i] + str(self.seed), wv.vector_size)\n",
    "\n",
    "        # Raise an error if an online update is run before initial training on a corpus\n",
    "        if not len(wv.vectors):\n",
    "            raise RuntimeError(\n",
    "                \"You cannot do an online vocabulary-update of a model which has no prior vocabulary. \"\n",
    "                \"First build the vocabulary of your model with a corpus before doing an online update.\"\n",
    "            )\n",
    "\n",
    "        wv.vectors = vstack([wv.vectors, newvectors])\n",
    "\n",
    "        if hs:\n",
    "            self.syn1 = vstack([self.syn1, zeros((gained_vocab, self.layer1_size), dtype=REAL)])\n",
    "        if negative:\n",
    "            pad = zeros((gained_vocab, self.layer1_size), dtype=REAL)\n",
    "            self.syn1neg = vstack([self.syn1neg, pad])\n",
    "        wv.vectors_norm = None\n",
    "\n",
    "        # do not suppress learning for already learned words\n",
    "        self.vectors_lockf = ones(len(wv.vocab), dtype=REAL)  # zeros suppress learning\n",
    "\n",
    "\n",
    "# Example: ./word2vec.py -train data.txt -output vec.txt -size 200 -window 5 -sample 1e-4 \\\n",
    "# -negative 5 -hs 0 -binary 0 -cbow 1 -iter 3\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s : %(threadName)s : %(levelname)s : %(message)s',\n",
    "        level=logging.INFO\n",
    "    )\n",
    "    logger.info(\"running %s\", \" \".join(sys.argv))\n",
    "    logger.info(\"using optimization %s\", FAST_VERSION)\n",
    "\n",
    "    # check and process cmdline input\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    if len(sys.argv) < 2:\n",
    "        print(globals()['__doc__'] % locals())\n",
    "        sys.exit(1)\n",
    "\n",
    "    from gensim.models.word2vec import Word2Vec  # noqa:F811 avoid referencing __main__ in pickle\n",
    "\n",
    "    seterr(all='raise')  # don't ignore numpy errors\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-train\", help=\"Use text data from file TRAIN to train the model\", required=True)\n",
    "    parser.add_argument(\"-output\", help=\"Use file OUTPUT to save the resulting word vectors\")\n",
    "    parser.add_argument(\"-window\", help=\"Set max skip length WINDOW between words; default is 5\", type=int, default=5)\n",
    "    parser.add_argument(\"-size\", help=\"Set size of word vectors; default is 100\", type=int, default=100)\n",
    "    parser.add_argument(\n",
    "        \"-sample\",\n",
    "        help=\"Set threshold for occurrence of words. \"\n",
    "             \"Those that appear with higher frequency in the training data will be randomly down-sampled;\"\n",
    "             \" default is 1e-3, useful range is (0, 1e-5)\",\n",
    "        type=float, default=1e-3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-hs\", help=\"Use Hierarchical Softmax; default is 0 (not used)\",\n",
    "        type=int, default=0, choices=[0, 1]\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-negative\", help=\"Number of negative examples; default is 5, common values are 3 - 10 (0 = not used)\",\n",
    "        type=int, default=5\n",
    "    )\n",
    "    parser.add_argument(\"-threads\", help=\"Use THREADS threads (default 12)\", type=int, default=12)\n",
    "    parser.add_argument(\"-iter\", help=\"Run more training iterations (default 5)\", type=int, default=5)\n",
    "    parser.add_argument(\n",
    "        \"-min_count\", help=\"This will discard words that appear less than MIN_COUNT times; default is 5\",\n",
    "        type=int, default=5\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-cbow\", help=\"Use the continuous bag of words model; default is 1 (use 0 for skip-gram model)\",\n",
    "        type=int, default=1, choices=[0, 1]\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-binary\", help=\"Save the resulting vectors in binary mode; default is 0 (off)\",\n",
    "        type=int, default=0, choices=[0, 1]\n",
    "    )\n",
    "    parser.add_argument(\"-accuracy\", help=\"Use questions from file ACCURACY to evaluate the model\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.cbow == 0:\n",
    "        skipgram = 1\n",
    "    else:\n",
    "        skipgram = 0\n",
    "\n",
    "    corpus = LineSentence(args.train)\n",
    "\n",
    "    model = Word2Vec(\n",
    "        corpus, size=args.size, min_count=args.min_count, workers=args.threads,\n",
    "        window=args.window, sample=args.sample, sg=skipgram, hs=args.hs,\n",
    "        negative=args.negative, cbow_mean=1, iter=args.iter\n",
    "    )\n",
    "\n",
    "    if args.output:\n",
    "        outfile = args.output\n",
    "        model.wv.save_word2vec_format(outfile, binary=args.binary)\n",
    "    else:\n",
    "        outfile = args.train\n",
    "        model.save(outfile + '.model')\n",
    "    if args.binary == 1:\n",
    "        model.wv.save_word2vec_format(outfile + '.model.bin', binary=True)\n",
    "    else:\n",
    "        model.wv.save_word2vec_format(outfile + '.model.txt', binary=False)\n",
    "\n",
    "    if args.accuracy:\n",
    "        model.accuracy(args.accuracy)\n",
    "\n",
    "    logger.info(\"finished running %s\", program)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
